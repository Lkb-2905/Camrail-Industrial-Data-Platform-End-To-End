ğŸŒ DOSSIER DE CONFIGURATION D'EXPLOITATION (DCE)
# âš¡ DPA : Data Pipeline Automation
![Terraform](https://img.shields.io/badge/Terraform-IaC-purple) ![Azure](https://img.shields.io/badge/Azure_Pipelines-CI/CD-blue) ![Kubernetes](https://img.shields.io/badge/Kubernetes-Data_Pods-blue) ![Grafana](https://img.shields.io/badge/Grafana-SRE_Dashboards-orange)

> âš ï¸ **NB IMPORTANT** : *Il s'agit d'un projet personnel/Ã©tudiant. Le nom "Camrail" est utilisÃ© uniquement pour donner un contexte industriel rÃ©aliste Ã  cette simulation. Aucune donnÃ©e rÃ©elle ou privÃ©e n'est exploitÃ©e. Tous les documents, architectures et donnÃ©es prÃ©sentÃ©s ici sont intÃ©gralement fictifs et crÃ©Ã©s de toutes piÃ¨ces pour simuler un projet acadÃ©mique de Data Engineering.*

**Version:** 1.0.0 Stable | **Date:** FÃ©vrier 2026  
**Auteur:** KAMENI TCHOUATCHEU GAETAN BRUNEL  
**Contact:** gaetanbrunel.kamenitchouatcheu@et.esiea.fr  

ğŸš€ [DÃ©marrage Rapide](#-dÃ©marrage-rapide) â€¢ ğŸ“š [Documentation](#-guide-dutilisation) â€¢ ğŸ¯ [FonctionnalitÃ©s](#-fonctionnalitÃ©s-clÃ©s) â€¢ ğŸ”§ [Installation](#-installation-rapide)

---

## ğŸ“‹ TABLE DES MATIÃˆRES
1. [Vue d'ensemble du projet](#-vue-densemble-du-projet)
2. [Architecture Technique](#ï¸-architecture-technique)
3. [Stack Technologique](#ï¸-stack-technologique)
4. [FonctionnalitÃ©s ClÃ©s](#-fonctionnalitÃ©s-clÃ©s)
5. [DÃ©marrage Rapide](#-dÃ©marrage-rapide)
6. [Guide d'Utilisation](#-guide-dutilisation)
7. [QualitÃ© & Best Practices](#-qualitÃ©--best-practices)
8. [Roadmap & Ã‰volutions](#ï¸-roadmap--Ã©volutions)

---

## ğŸ¯ VUE D'ENSEMBLE DU PROJET

### Contexte & Objectifs
Ce projet illustre l'implÃ©mentation robuste d'une architecture **Data-Driven (ETL)** pour le pilotage logistique des donnÃ©es de fret. Il rÃ©pond aux exigences d'une gouvernance informatique moderne en unifiant les donnÃ©es, les fichiers Ã©pars et les signaux mÃ©tiers.

Il illustre les compÃ©tences suivantes :

âœ… **Azure DevOps CI/CD :** Pipeline automatisÃ© des tests jusqu'au dÃ©ploiement (AKS).
âœ… **Infrastructure as Code (Terraform) :** Provisionnement complet et auditable de l'architecture Microsoft Azure.
âœ… **Kubernetes (AKS) :** Conteneurisation et auto-scaling horizontal des pods ETL.
âœ… **ObservabilitÃ© Grafana / Prometheus :** Dashboards complets d'analyse des flux de donnÃ©es.
âœ… **Streaming Asynchrone (Kafka) :** Les signaux logistiques sont bufferisÃ©s via topics.
âœ… **Data Warehouse Cloud (PostgreSQL) :** Stockage sÃ©curisÃ© et hautement performant sur Microsoft Azure.

### Pourquoi ce projet ?
| Aspect | DÃ©monstration |
| --- | --- |
| **ScalabilitÃ©** | L'Auto-Scaler Kubernetes multiplie les conteneurs ETL selon la charge de donnÃ©es arrivant dans Kafka. |
| **MaintenabilitÃ©** | L'Infrastructure `main.tf` (Terraform) permet de cloner l'environnement de production sur Azure en quelques minutes. |
| **Innovation** | Le CI/CD Azure Pipelines garantit 0 bug en production lors des dÃ©ploiements logistiques. |
| **SÃ©curitÃ©** | Gestion Cloud Azure sÃ©curisant les connexions Pods / Database via Secrets et authentifications fortes. |
| **Business Value** | Dote les gestionnaires de KPI calculÃ©s et requÃªtes avancÃ©es (Data Warehouse Azure). |

---

## ğŸ—ï¸ ARCHITECTURE TECHNIQUE

### Diagramme de Flux (Vue Logique & ETL Local)
```mermaid
flowchart TD
    classDef client fill:#38bdf8,stroke:#0284c7,stroke-width:2px,color:#000
    classDef app fill:#4ade80,stroke:#16a34a,stroke-width:2px,color:#000
    classDef intel fill:#facc15,stroke:#ca8a04,stroke-width:2px,color:#000
    classDef data fill:#f87171,stroke:#dc2626,stroke-width:2px,color:#fff
    classDef darkBox fill:#27272a,stroke:#52525b,stroke-width:2px,color:#fff

    subgraph Client_Layer["Client Layer"]
        O[ğŸ‘¤ DÃ©cideur Supply Chain]:::darkBox -->|Pilotage| R[Dashboard BI / DBeaver<br>SQL Analytics]:::client
    end

    subgraph Application_Layer["Application Layer"]
        N[Python ETL Pipeline<br>main_pipeline.py]:::app
        S[Data Warehouse<br>SQLite / SQLAlchemy]:::darkBox
        R -->|RequÃªtes SQL| S
        N -->|API Request| OM
        N -->|Fallback| SL
        N -->|Orchestration| S
    end

    subgraph Data_Sources["Data Sources"]
        OM[Kafka / API JSON<br>DonnÃ©es Cloud]:::data
        SL[Sources Locales<br>CSVs / JSONs / Excel]:::data
        SL -.-> OM
    end

    subgraph Intelligence_Layer["Intelligence Layer"]
        P[Python Transform<br>Pandas Analytics]:::intel
    end

    N -->|Shell Execution| P
    P -->|Data Output| N

    style Client_Layer fill:#3f3f46,stroke:#52525b,color:#fff
    style Application_Layer fill:#3f3f46,stroke:#52525b,color:#fff
    style Data_Sources fill:#3f3f46,stroke:#52525b,color:#fff
    style Intelligence_Layer fill:#3f3f46,stroke:#52525b,color:#fff
```

**RÃ©sultat visuel â€” Pipeline et DWH :**

Les deux captures ci-dessous s'affichent directement dans le README.

**1. ExÃ©cution du pipeline ETL** â€” Logs Extract / Transform / Load lors du lancement de `main_pipeline.py` :

![ExÃ©cution Pipeline ETL â€” DPA](../docs/screenshots/05_dpa_pipeline_execution.png)

**2. Base DWH SQLite** â€” Vue de la base `supply_chain_dwh.sqlite` dans DBeaver (tables `fact_transactions`, `aggr_daily_site_stats`) :

![Base DWH SQLite â€” DBeaver](../docs/screenshots/06_dpa_sqlite_dwh.png)

### Architecture Infra (Cloud)

Vue dâ€™ensemble du dÃ©ploiement sur Microsoft Azure (AKS, PostgreSQL, CI/CD).

```mermaid
flowchart LR
    subgraph Client
        U[ğŸ‘¤ DÃ©cideur]
        P[DBeaver / BI]
        U --> P
    end

    subgraph CICD["Azure DevOps CI/CD"]
        AZ[Build Â· Test Â· Push]
    end

    subgraph AKS["Azure Kubernetes Service"]
        K[Kafka]
        E[Extracteur]
        T[Transformateur]
        L[Worker SQL]
        PR[Prometheus]
        GF[Grafana]
        K --> E --> T --> L
        L --> PR --> GF
    end

    subgraph Infra
        TF[Terraform]
    end

    subgraph Data
        S[Terminaux Fret]
        D[(PostgreSQL)]
    end

    S --> K
    L --> D
    D --> P
    AZ --> TF
    TF --> AKS
```

### Flux de DonnÃ©es DÃ©taillÃ©
1. **Infrastructure as Code** : Terraform instancie Microsoft Azure PostgreSQL, Event Hubs et AKS.
2. **DÃ©ploiement CI/CD** : Azure DevOps compile et pousse l'image Docker ETL vers AKS pour son exÃ©cution en Pods.
3. **Extraction & Transformation (K8s)** : Les micro-services rÃ©cupÃ¨rent la donnÃ©e Kafka en direct et fusionnent les tables via Pandas.
4. **Chargement SQL & SRE** : Les donnÃ©es sont chargÃ©es en Bulk Upsert sur Azure Postgres. Prometheus et Grafana monitorent le flux de santÃ© absolu de l'architecture Big Data.

---

## ğŸ› ï¸ STACK TECHNOLOGIQUE

### Technologies Core
| Composant | Technologie | Version | Justification Technique |
| --- | --- | --- | --- |
| **Orchestrateur** | Python | 3.12+ | L'outil complet de traitement universel par batch d'ingÃ©nierie. |
| **Base de DonnÃ©es** | SQLite | 3+ | DWH de fichier ultra-lÃ©ger et transportable localement. |
| **Data Engine** | Pandas | Latest | Traitement et jointure rapide de la donnÃ©e en RAM. |
| **ORM et Mapping** | SQLAlchemy | Latest | ConnectivitÃ© et sÃ©curitÃ© des transactions SQL (pas de SQL syntax direct en paramÃ¨tre mÃ©tier). |

### BibliothÃ¨ques ComplÃ©mentaires
* **Loguru/Logging :** TraÃ§abilitÃ© exhaustive.
* **Openpyxl :** Lecture/Ã©criture Excel (source ERP, export rapports).
* **PyODBC :** Connexion Microsoft Access (bases legacy, migration).
* **OS/Sys :** Commandes natives utiles Ã  l'interaction Windows (Task Scheduler).

---

## ğŸ¯ FONCTIONNALITÃ‰S CLÃ‰S

### ğŸš€ FonctionnalitÃ©s Principales
**Gouvernance Data Temps RÃ©el**
* Consolidations des flux Ã©pars vers une Base Unique.
* CrÃ©ation propre et persistante de `supply_chain_dwh.sqlite`.

**SystÃ¨me AvancÃ© de RequÃªtage**
* DÃ©ploiement de scripts SQL analytiques poussÃ©s pour catÃ©goriser la fiabilitÃ© du systÃ¨me (Hub Logistics Classification).

**IntÃ©gration Excel / Access**
* **Source Excel :** Lecture de fichiers Excel comme alternative au CSV ERP via `extract_from_excel()`.
* **Export automatique :** Rapport multi-feuilles vers `reports/rapport_supply_chain.xlsx` Ã  chaque exÃ©cution.
* **Access :** Export pour import Access (migration mÃ©tier). Voir `exemples_excel_access/` pour les cas d'usage complets.

**Gestion des Risques**
* TolÃ©rance du Pipeline en mode fail-safe sur corruption partielle de fichier.

### ğŸ›¡ï¸ SÃ©curitÃ© & Robustesse
| Aspect | ImplÃ©mentation |
| --- | --- |
| **Validation** | VÃ©rification et parsing stricts avant transaction. |
| **RÃ©silience** | Base de donnÃ©es SQL protÃ©gÃ©e des deadlocks et Ã©checs (Rollbacks). |
| **TraÃ§abilitÃ©** | Logs dÃ©taillÃ©s sur serveurs ou scripts chronologiques. |

---

## ğŸš€ DÃ‰MARRAGE RAPIDE

### PrÃ©requis
* Python (v3.12+)

### Installation Rapide
```powershell
# 1. Cloner le projet (Naviguer au sein du rÃ©pertoire)
cd Data-Pipeline-Automation
pip install -r requirements.txt
cd src
python main_pipeline.py
```

### Lancement DÃ©veloppeur (Mode Local â€” RecommandÃ© pour dÃ©mo)

> ğŸ’¡ Utilisez le Python de **pyenv** si `python` ou `pip` ne sont pas configurÃ©s correctement.

```powershell
# 1. Installer les dÃ©pendances (pyenv recommandÃ©)
cd "c:\Users\pc\Desktop\projet CAMRAIL\Data-Pipeline-Automation"
& "$env:USERPROFILE\.pyenv\pyenv-win\versions\3.12.10\python.exe" -m pip install -r requirements.txt

# 2. Lancer l'Orchestrateur Complet (ETL)
cd src
& "$env:USERPROFILE\.pyenv\pyenv-win\versions\3.12.10\python.exe" main_pipeline.py
```

**AccÃ¨s ImmÃ©diat :** Les tables historiques sont fraÃ®ches et disponibles instantanÃ©ment dans `database/supply_chain_dwh.sqlite`. L'export Excel est gÃ©nÃ©rÃ© automatiquement dans `reports/rapport_supply_chain.xlsx`.

> ğŸ’¡ **Excel / Access :** Exemples et cas d'usage dans `exemples_excel_access/`. Depuis la racine du projet : `pip install -r exemples_excel_access/requirements.txt` puis `python exemples_excel_access/run_exemples.py`. Les sorties sont dans `exemples_excel_access/output/`.

---

## ğŸ“– GUIDE D'UTILISATION

### ScÃ©nario de Pilotage
1. **Lancement Quotidien :** L'outil tourne la nuit via scheduler.
2. **RequÃªtage Expert :** Un Data Analyst peut soumettre `sql/advanced_queries.sql` au DBeaver de la base.
3. **Action:** Construction de Dashboard et exports mÃ©tier sur l'activitÃ© des "Gares".

### Captures d'Ã‰cran

Chaque capture est affichÃ©e ci-dessous avec sa lÃ©gende.

**1. ExÃ©cution Pipeline ETL** â€” Logs Extract / Transform / Load lors du lancement de `main_pipeline.py` :

![ExÃ©cution Pipeline ETL â€” DPA](../docs/screenshots/05_dpa_pipeline_execution.png)

---

**2. Base DWH SQLite** â€” Vue de la base `supply_chain_dwh.sqlite` dans DBeaver (tables `fact_transactions`, `aggr_daily_site_stats`) :

![Base DWH SQLite â€” DBeaver](../docs/screenshots/06_dpa_sqlite_dwh.png)

> ğŸ’¡ Captures dans `docs/screenshots/` â€” Convention : voir `../docs/screenshots/README.md`

---

## âœ¨ QUALITÃ‰ & BEST PRACTICES

### Standards de Code
* **ModularitÃ© (Engine) :** Couches E, T, et L isolÃ©es nativement.
* **Typage (Data) :** Cast structurÃ©s au sein des Dataframes ; harmonisation des types (ex. `machine_id` en string) pour compatibilitÃ© source Excel au merge.
* **Error Handling :** Blocs robustes limitant l'Ã©crasement bdd en cas de corruption.

### MÃ©triques d'Excellence
âœ… **Architecture :** DWH Single Source of Truth respectÃ© (SSOT).
âœ… **Performance :** L'ORM insÃ¨re des dizaines de milliers de lignes en lots optimisÃ©s (bulk).

---

## ğŸ—ºï¸ ROADMAP & Ã‰VOLUTIONS

**Version Actuelle : 2.0.0 (Enterprise V2) âœ…**
* Architecture Data Streaming en Event-Driven via Apache Kafka.
* Virtualisation Globale : Orchestration via Docker Compose complet.
* Base de DonnÃ©es : Connecteur vers Cloud Azure PostgreSQL et Prometheus SRE.
* **IntÃ©gration Excel/Access :** Source Excel, export automatique vers `reports/rapport_supply_chain.xlsx`, exemples dans `exemples_excel_access/`.
* Documentation exhaustive DCE.

**Version 3.0.0 (Vision Long Terme) ğŸ”®**
* ImplÃ©mentation de Streaming Machine Learning Temps RÃ©el.

---

## ğŸ¤ CONTRIBUTION
Les contributions sont les bienvenues pour industrialiser ce socle mÃ©tier logistique.
1. Forker.
2. DÃ©finir une Feature Branche.
3. Valider par PR documentÃ©e.

---

## ğŸ“„ LICENCE
Ce projet est dÃ©veloppÃ© dans un cadre acadÃ©mique et professionnel. Droits rÃ©servÃ©s.

## ğŸ‘¨â€ğŸ’» AUTEUR
**KAMENI TCHOUATCHEU GAETAN BRUNEL**  
IngÃ©nieur Logiciel & Data Scientist en devenir | Ã‰tudiant ESIEA  

ğŸ“§ Email : gaetanbrunel.kamenitchouatcheu@et.esiea.fr  
ğŸ™ GitHub : @Lkb-2905  

ğŸ™ **REMERCIEMENTS**
* **Camrail / BollorÃ© Logistics :** Pour l'approche industrielle robuste Data Management.
* **ESIEA :** Pour l'excellence informatique et acadÃ©mique.

â­ Si ce projet vous semble pertinent pour la Supply Chain de demain, laissez une Ã©toile !  
Fait avec â¤ï¸, Python et du SQL.  

Â© 2026 Kameni Tchouatcheu Gaetan Brunel - Tous droits rÃ©servÃ©s
